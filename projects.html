<!DOCTYPE html>
<html>
<head>
<title>Projects</title>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<link rel="stylesheet" href="https://www.w3schools.com/w3css/4/w3.css"> 
<link rel='stylesheet' href='https://fonts.googleapis.com/css?family=Roboto'>
<!-- <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">   -->
<link rel="stylesheet" href="./resources/css/style.css">
<script src="https://kit.fontawesome.com/8add67db46.js" crossorigin="anonymous"></script>
</head>
<!-- <body class="w3-light-grey"> -->




<!-- Page Container -->
<div class="w3-content w3-margin-top" style="max-width:100%;">


    <!-- Nav bar -->
    <nav class="navbar">
        <ul>
            <li><a href="./index.html">About me</a></li>
            <li><a href="./projects.html">Projects</a></li>
            <li><a href="./blog.html">Blog</a></li>

        </ul>
    </nav>

    <!-- Banner -->

    <div class="banner">
        <h1> My Projects </h1>
    </div>

    <!----Projects Container-->
    <div class="project-container">

        <div class="project-box">
            <h2>Neural Networks in Python - LSTM</h2>
            <p>As a part of project in my Business Forecasting class, I created a Long Short-Term Memory model using the <code class="in-line-code">tensorflow</code> 
                package to predict temperatures in Louisiana. These predictions were then integrated with historic temperature data and then used in a model to predict 
                energy demand. The Jupyter notebook which documents the process of predicitng temperatures used can be found 
                <a href="https://github.com/mfritscher555/ISDS-7075-Project/blob/main/final_project/predictive_models/tensor_flow_temp.ipynb" target="_blank">here</a>.
            </p>
            <h2> Predicting Fuel Efficiency </h2>
        <p>For a project in my Operations Research class, I predicted 300 cars' fuel efficiency (in mpg). 
            The goal of the project was to minimize the Mean Absolute Percentage Error (MAPE). </p>
        <br/>
        <p>I was given a spreadsheet that contained a training dataset and a testing dataset. 
            I built a machine learning model using the training set, which was further split into a training/validation set.
            The MAPE of this model was around 6.1%. I then applied the model to make predictions on the testing set. 
            My final results achieved a MAPE of 6.7%, which was among the best scores ever achieved in the 10-year history of 
            this Operations Research class at LSU.</p>
        <br/>
        <p>I first had to process the data that was given to me. The training set did not require to data cleaning, but I recoded some of 
            the columns to make the analysis clearer. I also created dummy variables for the categorical column 'origin'. 
            The testing set required some data cleaning; this was done in the testing_set_cleaner.py file. Many of the functions require 
            "hard-coding" in correct values. The testing set was sufficiently small (at 300 observations) that data-wrangling with a 
            Python script was overkill (but good practice).</p>
        <br/>
        <p>In the Jupyter Notebook, I document each step in machine-learning process.</p>
        <br/>
        <p>To see my process, see my GitHub page <a href="https://github.com/mfritscher555/fuel_efficiency" target="_blank"> here.</a></p>
        <hr>
        <h2> Scraping YellowPages</h2> 
        <p>For my Python class in my Master's program, I created a web-scraping script. I decided to scrape Yellow Pages; I scraped 
            restaurants in New Orleans (it is always good to have a list, when you live somewhere with countless places to eat). 
            On my GitHub page you can see the script that I wrote. Click <a href="https://github.com/mfritscher555/yellowpages" target="_blank"> here</a>
            to see this project. </p>
        <br/>
        </body>
        



